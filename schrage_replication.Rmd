---
title: "Replication of Enos (2016)"
author: "Niel Schrage"
header-includes:
    - \usepackage{setspace}\doublespacing
date: "5/8/2020"
output: 
  bookdown::pdf_document2:
      latex_engine: xelatex
graphics: yes
bibliography: [schrage_bib.bib]
biblio-style: "apalike"
link-citations: false
toc: false
nocite: |
  @King06, @King00, @Enos14, @Fossett89, @Kinder96, @Craig18, @Dinesen15, @King95, @Amaral18,Johnston18, Florida20, @Enos16
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# loading relevant libraries

library(dplyr)
library(tidyverse)
library(ggplot2)
library(ggthemes)

library(bookdown)
library(tinytex)

library(gt)
library(gtsummary)
library(stargazer)

library(knitr)
library(styler)

library(rstanarm)
library(patchwork)

library(lubridate)



#library(boot)

options(tinytex.verbose = TRUE)


# setting up global values

# this package is about statistical estimates.
library(ei)
# package helpful for matching treated and control groups with similar covariate
# distributions.
library(MatchIt)
# package for producing simple weighted statistics
library(weights)
# simple bootstrapping -- didn't know this existed.
library(simpleboot)
# helpful statistics package, especially for modeling
library(Zelig)
# package that helps format latex objects side by side
library(apsrtable)

# Need to set wd so that my work is reproducible.

# so that I can crop
tinytex::tlmgr_install("pdfcrop")

# how to reference 

```

# Abstract
Enos (2016) measures the shift in voter turnout for white voters living in Chicago near demolished public housing, occupied predominantly by African Americans, as compared to white voters living farther away; observing that white voters living in close proximity to demolished public housing had a 10 percentage point drop in voter turnout between 2000 and 2004, Enos concludes that this change in behavior was the result of the decline in race threat from the change in size and proximity of the outgroup population. The results of my replication effort were largely successful, although there were some challenges. For my extension, I expanded the parallel trends robustness check that Enos presents in his appendix; my results were consistent with his findings. These results are significant in two important ways: first, they illustrate the strength of the robustness checks that Enos conducted and, second, they suggest that his conclusions about the effect of racial threat on voting are even more robust than his paper suggests.

# Introduction
Building on past research, Enos seeks to empirically test how much the context in which a person lives affects their political behavior. In 1949, V. O. Key published research suggesting increased political motivation by Southern whites threatened by the presence of African Americans; a relationship that is now known as “racial threat.” Enos studies this phenomenon by looking at public housing demolition in Chicago that caused mass displacement of aprroximately 25,000 African Americans. Enos examines the effect on white voting turnout in Chicago of removing African American neighbors. He tests this effect using a difference-in-differences model (which I will elaborate on more later). The effect of the treatment is the difference in average voter turnout at times t-1 (pre demolition turnout) and t (post demolition turnout) for white voters in close proximity to the demolished projects compared to the average voter turnout over the same period for white voters living farther away. The results suggest that racial threat is highly context specific and that the strength of the effect is inversely correlated with distance from the project and directly correlated with the size of the outgroup. 

I was largely successful in my attempt to replicate this paper. I’d like to thank Professor Enos for providing easily accessible [replication data](http://dvn.iq.harvard.edu/dvn/dv/ajps) through the [Harvard Dataverse](https://dataverse.harvard.edu/). I ran the replication using R [@R]. All of my code for this replication as well as my extension are in my repository located at ^[https://github.com/nschrage/schrage_replication] 

My extension expanded on the initial parallel trends test run by Enos as a robustness check to to validate the assumptions of the difference-in-differences model. For the difference-in-differences model he performed, Enos examined the change in voting turnout between 2000 and 2004 by African American and white treatment groups living near public housing projects that were demolished as compared against African American and white control groups that lived farther away. In order to illustrate the robustness of his results, he performed a parallel trends assumption test that included voting information for several different election cycles, and allows the trends for difference to be seen over time -- thus highlighting the effect of public housing demolition. If prior changes in voting turnout between 1996 - 2000 were similar for African American and white voters in both treatment and control, then the change in voting turnout behavior between 2000 - 2004 was unlikely to have been caused by an unmeasured change occurring prior to the demolition of the projects. I was inspired to do this extension for two reasons. First, this was an essential part of confirming the difference-in-differences model. Second, this was actually a potential replication that Enos himself suggested in the comments of his code as a worthwhile future project. 

To conduct this extension, I ran the parallel trends test code. Then I wrote new code to create graphics (displayed later) using Tidyverse [@Tidyverse]. Enos originally ran his test with distance set to 200 meters from demolished projects. To extend this, I ran a series of tests at 100, 200 (as a replication), and 300 meters. There are two important takeaways: First, by and large my figures confirmed Enos’ conclusions that the prior changes in voting were similar for white and black voters in both treatment and control. Second, it is essential to point out how the small sample size amongst the group of interest (white treatment) could be potentially skewing the results found here (and could potentially cast doubt on the difference-in-differences model.)

# Literature Review
In the paper “What the Demolition of Public Housing Teaches Us about the Impact of Racial Threat on Political Behavior,” Ryan Enos studies the impact that an exogenous demographic shift has on voting turnout and results. This paper seeks to answer the overarching question: how much does the context in which a certain person lives affect their political behavior? 

The paper seeks to extend the research performed by Key [@Key49] who examined county voting patterns in the American South and found a positive correlation between the number of African Americans in the county and white voter turnout in general and votes for conservative politicians in particular. Key argued that presence of African Americans -- the outgroup -- inspired fears among the white population, which motivated them to vote and vote for conservative politicians. He labeled this phenomenon “racial threat.” 

Key’s analysis launched a new field of research on racial context and its influence on behavior. The impact of context in general, and racial threat in particular, has blossomed into a cottage industry among the academic community. As Enos notes, studies have investigated, among other issues, the effects of racial threat on voter turnout [@Leighley99], candidate support [@Enos10], racial attitudes [@Gay06], and social capital [@Putnam07]. More recent studies have examined the effect of racial threat arising in the context of workplace populations [@Hamel20], influencing the election behavior of Latinos threatened by the candidacy of Donald Trump [@Gutierrez19], triggering support for ballot initiatives [@Reny18] and being shaped more by adolescent rather than contemporaneous context [@Goldman20].

However, while easy to hypothesize situations where context may be a powerful influence on attitudes and behavior, the challenge of empirically testing for racial threat has proven to be great. Enos highlights shortcomings in data, challenges of identification, and disputes over theory as historical limitations to contextual racial threat examinations. The variety of potential endogenous confounding factors that have historically made academic consensus on research and conclusions from context based racial threat studies hard to find.

For example, two studies highlighted by Enos, Giles and Buckner [@Giles93]) and Voss [@Voss96] illustrate the difficulties of identifying appropriate data to define context and assess whether racial threat is demonstrated. Using aggregate data on election results in the 1992 Louisiana gubernatorial election, Giles and Buckner concluded that the proximity of white voters to African American residents at the county level made whites more likely to vote for self avowed racist David Duke. They explained their result by suggesting that Duke’s candidacy triggered “old-fashioned” racial stereotypes. In contrast, Voss examined the same election but relied on a different geographic aggregation; they could find no relationship between white voters’ proximity to African Americans and votes for Duke’s election. Voss rejected the explanation of “old fashioned stereotypes” in current day Louisiana and argued that the use of aggregate data by Giles and Buckner was misplaced. 

Other scholars have highlighted alternate causal explanations, citing election bias as a challenge to establishing causation [@Cho13] or speculating that “economic threat” is a better explanation than “racial threat” [@Gay06]. Finally, the theory of context around “racial threat” has faced serious attack from scholars that, as Enos identifies, argue that “racial attitudes and related behaviors should not be sensitive to context.” He also identifies confusion over the mechanisms by which racial threat is manifest, identifying instrumental and psychological mechanisms as alternative explanations that may prove too much or too little. While this has long been a question political scientists have been interested in exploring, there are usually a number of obstacles to conducting a thorough investigation. 

Enos seeks to address these concerns by an innovative exploration of racial threat in the context of voting turnout. In the early 2000s, public housing reconstruction in Chicago caused mass displacement — forcing approximately 25,000 African Americans to move to new homes and new neighborhoods. Enos takes advantage of this unique situation to study racial threat and context. By relying on the “exogenous” nature of these demolitions -- the deciding factors about which projects would be demolished were based on algorithms outside of individual residents’ control -- Enos explores the effect of removing African American neighbors on white voting turnout in Chicago. 

Critically, Enos recognizes that the key assumption in this study is that the choice of units designated for demolition is uncorrelated with the difference in changes in turnout for white and African American voters. Enos describes how the demolitions can be thought of as a quasi -experiment, “The treatment is the demolition, and the outcome is the change in white political participation and support for conservative candidates.” 

For his study, Enos utilized four different data sources: voting information (consisting of an augmented 2004 Illinois voter file updated with 2000 and 2010 census data); location data between voters and their distance from each demolished housing project; edge data covering the exact distance between each voter and the edge of a housing project; and, lastly, property records for all Chicago voters. 

These data sources provide many advantages previously unavailable to studies of racial threat. In particular, the use of individual voter data avoids the ecological assumptions that usually plague aggregate/sampling data, and location data is helpful because it allows for testing of of a variety of definitions. The result is that any assessment of racial threat should be removed from the usual risks associated with data or causation that handicapped previous research. Similarly, because the change in the African American population is observable and triggered by exogenous activity -- the removal of the African American population as a result of the demolition of public housing -- the racial threat is more clearly identified by the changes in voter turnout and more clearly linked to the context in which it was observed.

Enos examined three hypotheses:
- H1 (Racial Threat and Turnout): After the demolition of the projects, turnout should decline for white voters close to the projects relative to the rest of the city.
- H2 (Proximity and Size): The salience of a group is a strong predictor of intergroup attitudes [@Brewer84]. Psychologists have empirically demonstrated the intuitive finding that salience can be a function of the size and “immediacy” of an object [@Latane95]. This leads me to expect a “dose effect,” whereby the treatment should vary with the size and proximity of the treatment. Operationally, the treatment effect should decline as the white voters are farther away from a project and as the population of a project represents a smaller portion of the local outgroup population.
- H3 (Racial Threat and Vote Choice): After the demolition of the projects, white voters close to the former projects should experience a decline in racially conservative voting relative to the rest of the city.

Enos tested these hypotheses using the difference-in-differences approach to examine changes in voter turnout between sets of voters (before and after the demolition) by race and proximity to the demolished projects.

The effect of the treatment (the demolition) is seen in difference in average voter turnout at times t-1 (pre demolition turnout) and t (post demolition turnout) for white voters in close proximity to the demolished projects compared to the average voter turnout over the same period for white voters living farther away. 

The equation that Enos uses:  

$ATE = [P(Vote2004|d∗ ≤ d) − P (Vote2000|d∗ ≤ d)] −[P(Vote2004|d∗ > d) − P(Vote2000|d∗ > d)]$

$d*$ is how Enos defines the treatment and control groups (who lives close v. far). He starts this value at 100 meters away and creates 10 different groups (so all the way up to 1000 meters or 1 kilometer which in his mind is sufficient for potential interactions.) For this study, the analysis suggests the following calculation: voting turnout among white treatment voters (those who live within a specified distance) in 2004 - 2000 minus the voting turnout of white control (those who live further than the specified distance) in the same years. 

Figure 1 displays this result for all four groups (African American Treatment, African American Control, White Treatment, White Control). 

In order to feel confident in the results found through the difference-in-differences test, Enos conducted a series of robustness checks. He identified that none were more important than the parallel trends test. By incorporating data from previous elections, Enos was able to illustrate that the previous trends in voting behavior between treatment and control groups were largely parallel -- meaning that the difference between the ‘treatment’ and ‘control’ groups stayed consistent over time [@Columbia], meaning that change observed is attributable to the treatment. 

The results suggest that racial threat is highly context specific and that the strength of the effect is inversely correlated with distance from the project and directly correlated with the size of the outgroup. These findings strongly suggest that racial threat occurs because of attitude change rather than selection.

# Replication 

![ADD CAPTION](data/images/fig1.png)

![ADD CAPTION](data/images/fig2a.png)

# Extension

# Appendix

```{r Setting Up Replication, echo=FALSE, message=FALSE, eval=FALSE}

# setting up global values

# this package is about statistical estimates.
library(ei)
# package helpful for matching treated and control groups with similar covariate
# distributions.
library(MatchIt)
# package for producing simple weighted statistics
library(weights)
# simple bootstrapping -- didn't know this existed.
library(simpleboot)
# helpful statistics package, especially for modeling
library(Zelig)
# package that helps format latex objects side by side
library(apsrtable)

### master graphic parameters for graphics
ylims <- c(-.35, .1)
ylims.2 <- c(-.45, .1)
xlims <- c(.5, 11)
# changing to see what happens 
dists <- seq(from = 1000, to = 100, by = -100) ### DELETE THIS LATER
xs <- seq(1:length(dists))
ys <- seq(from = -.35, to = .1, by = .05)
ys.lab <- c("-0.35", "-0.30", "-0.25", "-0.20", "-0.15", "-0.10", "-0.05", "0.00", "0.05", "0.10")
ys.2 <- seq(from = -.45, to = .1, by = .05)
ys.lab.2 <- c("-0.45", "-0.40", "-0.35", "-0.30", "-0.25", "-0.20", "-0.15", "-0.10", "-0.05", "0.00", "0.05", "0.10")

offsets <- .15
text.offsets <- .025
cex.axis <- .9
cex.N <- .7
top.text.adj <- c(1.3, 1.3) ## offsets on labels to reduce crowding
bottom.text.adj <- c(-.15, -.85)
point.size <- 2
line.offset <- .0175
```

```{r Figure 1 Estimation, echo = FALSE, cache = TRUE, eval=FALSE}

# loading data
x <- read_csv("data/data_clean/data.turnout.csv", col_types = cols(
  .default = col_double(),
  reg = col_date(format = ""),
  s = col_character()
))

# first I just want to replicate the results of fig 1

# creating some global variable 
# change later obviously
# dist <- 100

# I think I can use this later in the creation of some of the images. 
dists <- seq(from = 100, to = 1000, by = 100)



# don't really understand this...
namepcts = 1
  
  #c(seq(from = .91, to = .96, by = .01),.975,.99,1)

##matrices for stroing results
res.mat = matrix(nrow=length(namepcts),ncol=length(dists))

white.treat.N = res.mat
white.treat.effect.mean.boot = res.mat
white.treat.effect.conf.boot.lower = res.mat
white.treat.effect.conf.boot.upper = res.mat

black.treat.N = res.mat
black.treat.effect.mean.boot = res.mat
black.treat.effect.conf.boot.lower = res.mat
black.treat.effect.conf.boot.upper = res.mat


# where do the estimations come from
# figure out how to run the regressions he does... 
# makes more sense to just translate from base r, don't want to deal with drama again... 

# creating treatment and control groups, adapted from enos code.

##loop through definitions of white and distances and estimate at each combination
for(j in 1:length(namepcts)){
	##define a treatment and control group for each name percent
	white = x[x$whitename>=namepcts[j],]
   black = x[x$blackname>=namepcts[j],]
  
    for(h in 1:length(dists)){
      	treatment_white = x[x$demo.distance<=dists[h],]
      	treatment_black = x[x$demo.distance<=dists[h],]
      	control_white = x[x$demo.distance>dists[h],]
      	control_black = x[x$demo.distance>dists[h],]     		
	
      	white.treat.N[j,h] = nrow(treatment_white)
      	black.treat.N[j,h] = nrow(treatment_black)
	      	
	   ##for white and black subjects, perform t test of differences of means with boostrapped standard errors  	
		if(white.treat.N[j,h] > 0){
			white.boot = two.boot((treatment_white$vote2004-treatment_white$vote2000),(control_white$vote2004-control_white$vote2000),mean, R = 1000, na.rm=T)
			white.treat.effect.mean.boot[j,h] = white.boot$t0
			white.boot.ci = boot.ci(white.boot, type = 'basic')
			white.treat.effect.conf.boot.lower[j,h] = white.boot.ci$basic[4]
			white.treat.effect.conf.boot.upper[j,h] = white.boot.ci$basic[5]
		      		}
		      		
		if(black.treat.N[j,h] > 0){
			black.boot = two.boot((treatment_black$vote2004-treatment_black$vote2000),(control_black$vote2004-control_black$vote2000),mean, R = 1000, na.rm=T)
			black.treat.effect.mean.boot[j,h] = black.boot$t0
			black.boot.ci = boot.ci(black.boot, type = 'basic')
			black.treat.effect.conf.boot.lower[j,h] = black.boot.ci$basic[4]
			black.treat.effect.conf.boot.upper[j,h] = black.boot.ci$basic[5]		
			 }
			 }
	}
# I know there is some iteration that I can do that would make my life so much easier
# had
# so is this the model being run
# in his words... 
# i'm having some trouble bootstrapping confidence intervals. 
# where does the turn out stuff come into play
## for white and black subjects, perform t test of differences of means with
## boostrapped standard errors
  
# try to recreate graphic #1. 

# also not sure if I thought this through... ran my computer all night so that I could get 

```

```{r Figure 1 Replication, echo = FALSE, results="asis", eval=FALSE}

# how to display results.
# how to only display graph 1, not the appendix stuff
## load data
# ALSO WHY DO THE GRAPHS APPEAR IN SEPARATE PDFS, HOW CAN I GET THEM TO APPEAR WITHIN THIS MARKDOWN DOCUMENT?

wtreat <- read.csv("data/data_clean/white.treat.effect.mean.boot.csv")

wtreat.lower <- read.csv("data/data_clean/white.treat.effect.conf.boot.lower.csv")

wtreat.upper <- read.csv("data/data_clean/white.treat.effect.conf.boot.upper.csv")

Nwtreat <- read.csv("data/data_clean/white.treat.N.csv")

btreat <- read.csv("data/data_clean/black.treat.effect.mean.boot.csv")

btreat.lower <- read.csv("data/data_clean/black.treat.effect.conf.boot.lower.csv")

btreat.upper <- read.csv("data/data_clean/black.treat.effect.conf.boot.upper.csv")

Nbtreat <- read.csv("~/Desktop/Gov_1006_Projects/replication_2/dataverse_files/black.treat.N.csv")

## letters for marking graphs, one is not used
 #use.letters <- c("a", "b", "c", "d", "e", "f", "skip", "g", "h")

## cycle through each line of data, each of which are groups defined by diferent namepcts

#for (i in 1:nrow(wtreat)) 
  { ## turning into matrices helps below with segment function
    
  use.wtreat <- as.matrix(wtreat[7, ])
  use.wlower <- as.matrix(wtreat.lower[7, ])
  use.wupper <- as.matrix(wtreat.upper[7, ])
  use.Nwtreat <- as.matrix(Nwtreat[7, ])
  use.btreat <- as.matrix(btreat[7, ])
  use.blower <- as.matrix(btreat.lower[7, ])
  use.bupper <- as.matrix(btreat.upper[7, ])
  use.Nbtreat <- as.matrix(Nbtreat[7, ])

  # how to just get figure_1.pdf to display
  # also how to get the pdf to display in my document?

  ## name graphs
  # if (i == 7) {
  #   
  #   pdf("Figure_1.pdf")
  # }
   
  # else {
  #   pdf(paste("Figure_A1", use.letters[i], ".pdf", sep = ""))
  # }
  
  
  par(las = 1)
  
  par(mar = c(5.1, 4.1, .5, .5))
 
  x <- plot(xs, use.wtreat,
    ylim = ylims,
    xlim = xlims,
    type = "n",
    ylab = "Treatment Effect",
    xlab = "Treated Group Distance from Projects",
    xaxt = "n",
    yaxt = "n.csv"
  )
  
  ## ZERO LINE
  
  abline(h = 0, lty = 2)

  ### draw lines first because I want them to be covered by points
  #### create spaces in lines using the offset (this allows the N to be displayed with the text() function)
  
  ## black lines are offset to the left, white lines to the right
  segments(
    x0 = xs[1:2] + offsets, x1 = xs[1:2] + offsets, ## only do it for low N blacks because otherwise lines look funny
    y0 = use.btreat[, 1:2], y1 = use.blower[, 1:2]
  )
  
  segments(
    x0 = xs[1:2] + offsets, x1 = xs[1:2] + offsets,
    y0 = use.btreat[, 1:2] + line.offset, y1 = use.bupper[, 1:2]
  )
  
  ## now the others
  segments(
    x0 = xs[3:10] + offsets, x1 = xs[3:10] + offsets,
    y0 = use.blower[, 3:10], y1 = use.bupper[, 3:10]
  )

  segments(
    x0 = xs - offsets, x1 = xs - offsets, ## bottomlines
    y0 = use.wtreat - line.offset, y1 = use.wlower
  )

  segments(
    x0 = xs - offsets, x1 = xs - offsets, ## toplines
    y0 = use.wtreat, y1 = use.wupper
  )


  ## points and N descriptions
  
  ## WHITE DOT POINTS
  points(xs - offsets, use.wtreat,
    cex = point.size,
    pch = 21,
    bg = "white",
    col = "black"
  )
  
  ## WHITE DOT LABELS
  text(xs - offsets, use.wtreat,
    paste("(", use.Nwtreat, ")", sep = ""),
    cex = cex.N,
    adj = top.text.adj,
    pos = 1
  )
  
  ## BLACK Dot Points

  points(xs + offsets, use.btreat,
    pch = 16,
    cex = point.size
  )
  
  ## Black Dot Lables
  
  text(xs + offsets, use.btreat,
    paste("(", use.Nbtreat, ")", sep = ""),
    cex = cex.N,
    adj = bottom.text.adj,
    pos = 3
  )
  
  ## X AXIS Controls

  axis(
    side = 1,
    at = xs,
    label = seq(100, 1000, 100),
    cex.axis = cex.axis
  )
  
  ## Y AXIS Controls
  
  axis(
    side = 2,
    at = ys,
    label = ys.lab,
    cex.axis = cex.axis
  )

  dev.print()
}


x


```

```{r Figures 2-3 Replication, echo=FALSE, results = "asis", eval=FALSE}

## this cycles thorugh a bunch of dataframes, each of which is needed for a different graph
for (figure in c( "white.demo.main", "blackmain")) {

  # if (figure == "white.basic.main")
  # {
  # 
  #    ## this group is different than the rest because the second set is not actually a diff in diff, but calling it "diffs" for consistency
  #   treat <- read.csv("data/data_clean/white.match.basic.csv")
  #   #treat.2 <- read.csv("data/data_clean/white.match.basic.property.csv")
  #   #fig.nums <- c("A3", "A4") ## figure names
  #   pchs <- c(17, 17) ## point types
  # }
  
  if (figure == "white.demo.main") 
  {
    treat <- read.csv("data/data_clean/white.match.nondemolished.csv")
    diffs <- read.csv("data/data_clean/white.match.nondemolished.diffs.csv")
    fig.nums <- c("2")
    pchs <- c(17)
  }

  # if (figure == "white.demo.property") {
  #   treat <- read.csv("~/Desktop/Gov_1006_Projects/replication_2/dataverse_files/white.match.nondemolished.property.csv")
  #   diffs <- read.csv("~/Desktop/Gov_1006_Projects/replication_2/dataverse_files/white.match.nondemolished.diffs.property.csv")
  #   fig.nums <- c("A6", "A7")
  #   pchs <- c(17, 22)
  # }

  # if (figure == "white.demo.localrace") 
  # {
  #   treat <- read.csv("data/data_clean/white.match.nondemolished.localrace.csv")
  #   diffs <- read.csv("data/data_clean/white.match.nondemolished.diffs.localrace.csv")
  #   fig.nums <- c("A8", "A9")
  #   pchs <- c(17, 22)
  # }
  
  if (figure == "blackmain") 
  {
    treat <- read.csv("data/data_clean/white.match.black.property.csv")
    diffs <- read.csv("data/data_clean/white.match.black.diffs.property.csv")
    fig.nums <- c("3")
    pchs <- c(17)
  }
  
  # if (figure == "blackcensus") 
  # {
  #   treat <- read.csv("data/data_clean/white.match.black.csv")
  #   diffs <- read.csv("data/data_clean/white.match.black.diffs.csv")
  #   fig.nums <- c("A10", "A11")
  #   pchs <- c(17, 21)
  # }

  ## define axis for different graphs
 
   if (figure %in% c("white.basic.main", "white.demo.main", "blackmain")) 
  {
    use.ylims <- ylims
    use.ys.lab <- ys.lab
    use.ys <- ys
  }
  
  else 
  {
    use.ylims <- ylims.2
    use.ys.lab <- ys.lab.2
    use.ys <- ys.2
  }

  # go through pairs for each pair of dataframe
  for (i in 1)
  {
    if (i == 1)
    {
      use.treat <- treat$coefficient
      clower <- use.treat - (1.96 * treat$stdev)
      cupper <- use.treat + (1.96 * treat$stdev)
      use.N.treat <- treat$N.treatment + treat$N.control
    }

    if (i == 2 & figure != "white.basic.main")
    {
      use.treat <- diffs$mean.diff
      clower <- diffs$low.ci
      cupper <- diffs$high.ci
      use.N.treat <- diffs$N
    }

    # if (i == 2 & figure == "white.basic.main") 
    # { ## white.basic.main figures have slightly different structure
    #   use.treat <- treat.2$coefficient
    #   clower <- use.treat - (1.96 * treat.2$stdev)
    #   cupper <- use.treat + (1.96 * treat.2$stdev)
    #   use.N.treat <- treat.2$N.treatment + treat.2$N.control
    # }

    # if (figure %in% c("white.demo.main", "blackmain") & i == 1) 
    # {
    #   pdf(paste("Figure_", fig.nums[i], ".pdf", sep = ""))
    # 
    # }
    # else 
    # {
    #   pdf(paste("Figure_", fig.nums[i], ".pdf", sep = ""))
    # }
    
    par(las = 1)
    
    par(mar = c(5.1, 4.1, .5, .5))
    
    plot(xs, use.treat,
      ylim = use.ylims,
      xlim = xlims,
      type = "n",
      ylab = "Treatment Effect",
      xlab = "Treated Group Distance from Projects",
      xaxt = "n",
      yaxt = "n"
    )
    
    # ZERO LINE
    
    abline(h = 0, lty = 2)

    # UPPER TREND LINE
    segments(
      x0 = xs, x1 = xs,
      y0 = use.treat + line.offset, y1 = cupper
    )
    
    # LOWER TREND LINE
    segments(
      x0 = xs, x1 = xs,
      y0 = use.treat, y1 = clower
    )

    ## Treatment Effects
    points(xs, use.treat,
      pch = pchs[i],
      cex = point.size,
      bg = "white",
      col = "black"
    )
    
    text(xs, use.treat,
      paste("(", use.N.treat, ")", sep = ""),
      cex = cex.N,
      pos = 3
    )
    
    # X AXIS LABEL
    
    axis(
      side = 1,
      at = xs,
      label = seq(100, 1000, 100),
      cex.axis = cex.axis
    )
    
    # Y AXIS LABEL
    
    axis(
      side = 2,
      at = use.ys,
      label = use.ys.lab,
      cex.axis = cex.axis
    )

    #dev.print()
  }
}

# black triange 64 = 2
# black triangle 74 = 3

```

```{r Figure 4 Replication, echo = FALSE, results="asis", fig.align='center', eval=FALSE}

## write out regression to latex table

# THIS IS SOMETHING THAT DOESN'T WORK, BUT IT IS SOMETHING THAT I CAN WRITE ABOUT FOR ONE OF THE QUESTIONS ON THE MILESTONE. FOR FIGURE 4, THE TABLE WITH THE REGRESSION WAS UNABLE TO LOAD, UPON FURTHER ANALYSIS, IT LOOKS LIKE THE .RDA FILE DOES NOT EXIST IN THE DATA VERSE FOLDER. TO MAKE SURE, I ALSO CHECKED ONLINE AND THIS WAS ALSO THE CASE, NOT SURE WHY THIS IS OR IF IT STOPS US FROM GETTING SOME OF THE OTHER DATA/FIGURES.

# load('~/Desktop/Gov_1006_Projects/replication_2/dataverse_files/out.reg.predictions.rda') ##load saved model
# out.model = apsrtable(out.reg.predictions,
# 	coef.names = c( 'Intercept','log(distance)','log(percent of local black population)','2000 turnout'),
# 	digits = 3
# 	)
# writeLines(out.model,
# 	'Table_1.tex')

distdat <- read.csv("data/data_clean/predicted.results.distance.vary.context.csv")
areadat <- read.csv("data/data_clean/predicted.results.area.vary.context.csv")

## new ylims for these graphs
ylims.predict <- c(.6, .75)

datas <- list(distdat, areadat) ## put data in a list to cycle through
## parameters to be used in graphs below
xs <- list(seq(from = 10, to = 2000, by = 10), seq(from = 45000, to = 1004000, by = 4800) / 1000)
use.letters <- c("a", "b")
xlabs <- c("Distance from Project", "Percent of Local Black Population in Demolished Project")
ylabs <- c(expression(Pr(vote[2004])), "")
vlines <- list(seq(from = 0, to = 2000, by = 200), seq(from = 0, to = 1000, by = 100))
axis.labs <- list(
  as.character(seq(from = 0, to = 2000, by = 200)),
  as.character(c("0", "10%", "20%", "30%", "40%", "50%", "60%", "70%", "80%", "90%", "100%"))
)

for (i in 1:2) {
  colnames(datas[[i]]) <- c("mean", "sd", "50%", "2.5%", "97.5%") ## saving renames columns, so name back

  # png(paste("Figure_4", use.letters[i], ".png", sep = ""))
  par(las = 1)
  par(mar = c(5.1, 4.1, .5, .5))
  plot(xs[[i]], datas[[i]][, "mean"],
    type = "l",
    xlab = xlabs[i],
    ylab = ylabs[i],
    ylim = ylims.predict,
    xaxt = "n",
    cex.axis = cex.axis,
    lwd = 4
  )
  ## put horizontal and vertical lines on plots
  abline(
    h = seq(from = min(ylims.predict), to = max(ylims.predict), by = .025),
    lty = 2,
    col = "gray",
    lwd = 1
  )
  abline(
    v = vlines[[i]],
    lty = 2,
    col = "gray",
    lwd = 1
  )
  lines(xs[[i]], datas[[i]][, "2.5%"],
    lty = 3,
    lwd = 2.5
  )
  lines(xs[[i]], datas[[i]][, "97.5%"],
    lty = 3,
    lwd = 2.5
  )
  axis(
    side = 1,
    at = vlines[[i]],
    labels = axis.labs[[i]],
    cex.axis = cex.axis
  )
  

  #dev.off()
}


```

```{r Figures 5-6 Replication, echo = FALSE, results="asis", eval=FALSE}

pres.elections <- c("dole_pct_ei", "bush2000_pct_ei", "bush2004_pct_ei", "mccain_pct_ei")
obama.elections <- c("obama_sen_primary_pct_ei", "keyes_pct_ei", "obama_pres_primary_pct_ei")


dists <- read.csv("data/data_clean/distance.vote.differences.csv")
demos <- read.csv("data/data_clean/demolished.vote.differences.csv")


graphs <- c("5a", "5b", "6")

for (i in graphs){
  
  if (i == "5a"){
    dat <- dists
  }
  
  else {
    dat <- demos
  }

  if (i %in% c("5a", "5b")) {
    xlims <- c(.75, 4.25)
    ylims <- c(-.1, .2)
  }
  
  else {
    xlims <- c(.75, 3.25)
    ylims <- c(-.1, .25)
  }

  ## recode Keyes to Obama general for presentation purposes
  dat[dat$election == "keyes_pct_ei", "x.mean"] <- 1 - dat[dat$election == "keyes_pct_ei", "x.mean"]
  dat[dat$election == "keyes_pct_ei", "y.mean"] <- 1 - dat[dat$election == "keyes_pct_ei", "y.mean"]
  dat[dat$election == "keyes_pct_ei", "diff"] <- dat[dat$election == "keyes_pct_ei", "y.mean"] - dat[dat$election == "keyes_pct_ei", "x.mean"]

  # pdf(paste("Figure_", i, ".pdf", sep = ""),
  #   width = 7, height = 8
  # )
  
  par(las = 1)
  par(mar = c(5.1, 4.1, .5, 1.5))
  plot(seq(1:4),
    rep(1, 4),
    ylim = ylims,
    xlim = xlims,
    type = "n",
    xaxt = "n",
    yaxt = "n",
    xlab = "Election",
    ylab = ifelse(i == "5b", "", "Treatment Effect")
  )
  
  abline(h = 0, lty = 2)

  if (i %in% c("5a", "5b")) {
    segments(
      x0 = seq(1:4) - offsets,
      x1 = seq(1:4) - offsets,
      y0 = dat[dat$group == "white" & dat$election %in% pres.elections, "diff"] - (1.96 * dat[dat$group == "white" & dat$election %in% pres.elections, "sd"]),
      y1 = dat[dat$group == "white" & dat$election %in% pres.elections, "diff"] + (1.96 * dat[dat$group == "white" & dat$election %in% pres.elections, "sd"])
    )
    points(seq(1:4) - offsets,
      dat[dat$group == "white" & dat$election %in% pres.elections, "diff"],
      pch = 21,
      bg = "white",
      col = "black",
      cex = 2
    )
    segments(
      x0 = seq(1:4) + offsets,
      x1 = seq(1:4) + offsets,
      y0 = dat[dat$group == "black" & dat$election %in% pres.elections, "diff"] - (1.96 * dat[dat$group == "black" & dat$election %in% pres.elections, "sd"]),
      y1 = dat[dat$group == "black" & dat$election %in% pres.elections, "diff"] + (1.96 * dat[dat$group == "black" & dat$election %in% pres.elections, "sd"])
    )
    points(seq(1:4) + offsets,
      dat[dat$group == "black" & dat$election %in% pres.elections, "diff"],
      pch = 16,
      cex = 2
    )
    axis(
      side = 1, at = seq(1:4),
      c("1996", "2000", "2004", "2008"),
      tick = F,
      cex.axis = cex.axis
    )
  }
  else {
    segments(
      x0 = seq(1:3) - offsets,
      x1 = seq(1:3) - offsets,
      y0 = dat[dat$group == "white" & dat$election %in% obama.elections, "diff"] - (1.96 * dat[dat$group == "white" & dat$election %in% obama.elections, "sd"]),
      y1 = dat[dat$group == "white" & dat$election %in% obama.elections, "diff"] + (1.96 * dat[dat$group == "white" & dat$election %in% obama.elections, "sd"])
    )
    points(seq(1:3) - offsets,
      dat[dat$group == "white" & dat$election %in% obama.elections, "diff"],
      pch = 21,
      bg = "white",
      col = "black",
      cex = 2
    )
    
    segments(
      x0 = seq(1:3) + offsets,
      x1 = seq(1:3) + offsets,
      y0 = dat[dat$group == "black" & dat$election %in% obama.elections, "diff"] - (1.96 * dat[dat$group == "black" & dat$election %in% obama.elections, "sd"]),
      y1 = dat[dat$group == "black" & dat$election %in% obama.elections, "diff"] + (1.96 * dat[dat$group == "black" & dat$election %in% obama.elections, "sd"])
    )
    
    points(seq(1:3) + offsets,
      dat[dat$group == "black" & dat$election %in% obama.elections, "diff"],
      pch = 16,
      cex = 2
    )
    
    axis(
      side = 1, at = seq(1:3),
      c("2004 \n Senate Primary", "2004 \n Senate General", "2008 \n President Primary"),
      tick = F,
      cex.axis = cex.axis
    )
  }
  
  axis(
    side = 2,
    at = seq(from = -.1, to = .3, by = .05),
    label = c("-0.10", "-0.05", "0.00", "0.05", "0.10", "0.15", "0.20", "0.25", "0.30"),
    cex.axis = cex.axis
  )
  
  #dev.off()
}
```

```{r "Figure A2 2Replication of Parallel Trends D200 Extension", echo=FALSE, results="asis", eval=FALSE}

# loading data
x <- read_csv("data/data_clean/data.turnout.csv", col_types = cols(
  .default = col_double(),
  reg = col_date(format = ""),
  s = col_character()
))

##these are the elections to look at
elections = c('vote1996','vote1998','vote2000','vote2002','vote2004')

##matrices for storing results
outmat = matrix(nrow=length(elections), ncol=4)
colnames(outmat) = c('white.treatment','white.control','black.treatment','black.control')

##use different registration cutoff here because going all the way back to 1996

# COULD JUST LOAD DATA HERE...

x <- x %>% 
  
  filter(reg < "1996-10-08") %>%
  
  filter(is.na(reg) == F)


##define a treatment and control group for each name percent
useW = x[x$whitename>=.975,]
useB = x[x$blackname>=.975,]

##set distance for parallel trends test to 200 meters, can be tested at other distances too (MAYBE THIS IS MY PROJECT)  
treatment_white = useW[useW$demo.distance <= 200,]
treatment_black = useB[useB$demo.distance <= 200,]
control_white = useW[useW$demo.distance > 200,]
control_black = useB[useB$demo.distance > 200,]     		

WtreatN = nrow(treatment_white)
BtreatN = nrow(treatment_black)
WcontN = nrow(control_white)
BcontN = nrow(control_black)
     
##test turnout across difference elections     
for(i in 1:length(elections)){
		election = elections[i]
		outmat[i,'white.treatment'] = sum(treatment_white[election],na.rm=T)/WtreatN
	  outmat[i,'black.treatment'] = sum(treatment_black[election],na.rm=T)/BtreatN
	  outmat[i,'white.control'] = sum(control_white[election],na.rm=T)/WcontN
	  outmat[i,'black.control'] = sum(control_black[election],na.rm=T)/BcontN  
}


# Data Wrangling -- Creating Tibble
	    
parallel.trends_100 = outmat

parallel.trends_100 <- as.tibble(parallel.trends_100) %>% 
  
  rename("WT" = "white.treatment","WC" = "white.control", "BT" = "black.treatment", "BC" = "black.control")  

year <- c('1996','1998','2000','2002','2004')  
  
year <- as.tibble(year) 

parallel.trends_100  <- tibble(parallel.trends_100, year)

# Building Basic Graphic 

parallel.trends_100_graphic <- parallel.trends_100  %>% 
  
  ggplot(aes(x = value)) +
  
  geom_text((aes(y = WT, label = "WT"))) +

  geom_text((aes(y = WC, label = "WC"))) +
  
  geom_text((aes(y = BT, label = "BT"))) +
   
  geom_text((aes(y = BC, label = "BC"))) +

  # from = .5, to = .9, by = .05
  
  scale_y_continuous(breaks = c(0.5, 0.55, 0.60, 0.65, 0.70, 0.75, 0.8, 0.85, 0.9), labels = c(0.5, 0.55, 0.60, 0.65, 0.70, 0.75, 0.8, 0.85, 0.9), name = "Percent Voting Turnout", limits = c(0.5,0.9)) + 
  
  scale_x_discrete(breaks = c(1996, 1998, 2000, 2002, 2004), labels = c(1996, 1998, 2000, 2002, 2004), name = "Year") +
  
  labs(title = "Parallel Trends Test", subtitle = "D = 200 meters") +
  
  # geom_vline(xintercept = "2002", color = "red") +
  
  # potentially do an annotate 
  
  theme_classic()  
  
parallel.trends_100_graphic_1 <- parallel.trends_100_graphic
bonjour <- c(1,2,3,4)
parallel.trends_100 <- as.data.frame(parallel.trends_100)

# Getting the Lines to Show Up, Shouldn't Have Been Its Own Thing BUT HERE WE ARE

for (j in bonjour) {
  for(i in bonjour){
     
     parallel.trends_100_graphic_1 <- parallel.trends_100_graphic_1 +  
     
    geom_segment(x = parallel.trends_100$value[i], y = parallel.trends_100[i, j], 
                 xend = parallel.trends_100$value[i+1], yend = parallel.trends_100[i+1, j])
     
  }
  
  parallel.trends_100_graphic_1
}
 
parallel.trends_100_graphic_1


```

```{r Parallel Trends D100 Extension, echo=FALSE, results="asis", fig.cap="TEST", eval=FALSE}

# loading data
x <- read_csv("data/data_clean/data.turnout.csv", col_types = cols(
  .default = col_double(),
  reg = col_date(format = ""),
  s = col_character()
))

##these are the elections to look at
elections = c('vote1996','vote1998','vote2000','vote2002','vote2004')

##matrices for storing results
outmat = matrix(nrow=length(elections), ncol=4)
colnames(outmat) = c('white.treatment','white.control','black.treatment','black.control')

##use different registration cutoff here because going all the way back to 1996

# COULD JUST LOAD DATA HERE...

x <- x %>% 
  
  filter(reg < "1996-10-08") %>%
  
  filter(is.na(reg) == F)


##define a treatment and control group for each name percent
useW = x[x$whitename>=.975,]
useB = x[x$blackname>=.975,]

##set distance for parallel trends test to 200 meters, can be tested at other distances too (MAYBE THIS IS MY PROJECT)  
treatment_white = useW[useW$demo.distance <= 100,]
treatment_black = useB[useB$demo.distance <= 100,]
control_white = useW[useW$demo.distance > 100,]
control_black = useB[useB$demo.distance > 100,]     		

WtreatN = nrow(treatment_white)
BtreatN = nrow(treatment_black)
WcontN = nrow(control_white)
BcontN = nrow(control_black)
     
##test turnout across difference elections     
for(i in 1:length(elections)){
		election = elections[i]
		outmat[i,'white.treatment'] = sum(treatment_white[election],na.rm=T)/WtreatN
	  outmat[i,'black.treatment'] = sum(treatment_black[election],na.rm=T)/BtreatN
	  outmat[i,'white.control'] = sum(control_white[election],na.rm=T)/WcontN
	  outmat[i,'black.control'] = sum(control_black[election],na.rm=T)/BcontN  
}


# Data Wrangling -- Creating Tibble
	    
parallel.trends_100 = outmat

parallel.trends_100 <- as.tibble(parallel.trends_100) %>% 
  
  rename("WT" = "white.treatment","WC" = "white.control", "BT" = "black.treatment", "BC" = "black.control")  

year <- c('1996','1998','2000','2002','2004')  
  
year <- as.tibble(year) 

parallel.trends_100  <- tibble(parallel.trends_100, year)

# Building Basic Graphic 

parallel.trends_100_graphic <- parallel.trends_100  %>% 
  
  ggplot(aes(x = value)) +
  
  geom_text((aes(y = WT, label = "WT"))) +

  geom_text((aes(y = WC, label = "WC"))) +
  
  geom_text((aes(y = BT, label = "BT"))) +
   
  geom_text((aes(y = BC, label = "BC"))) +

  # from = .5, to = .9, by = .05
  
  scale_y_continuous(breaks = c(0.5, 0.55, 0.60, 0.65, 0.70, 0.75, 0.8, 0.85, 0.9), labels = c(0.5, 0.55, 0.60, 0.65, 0.70, 0.75, 0.8, 0.85, 0.9), name = "Percent Voting Turnout", limits = c(0.5,0.9)) + 
  
  scale_x_discrete(breaks = c(1996, 1998, 2000, 2002, 2004), labels = c(1996, 1998, 2000, 2002, 2004), name = "Year") +
  
  labs(title = "Parallel Trends Test", subtitle = "D = 100 meters") +
  
  # geom_vline(xintercept = "2002", color = "red") +
  
  # potentially do an annotate 
  
  theme_classic()  
  
parallel.trends_100_graphic_1 <- parallel.trends_100_graphic
bonjour <- c(1,2,3,4)
parallel.trends_100 <- as.data.frame(parallel.trends_100)

# Getting the Lines to Show Up, Shouldn't Have Been Its Own Thing BUT HERE WE ARE

for (j in bonjour) {
  for(i in bonjour){
     
     parallel.trends_100_graphic_1 <- parallel.trends_100_graphic_1 +  
     
    geom_segment(x = parallel.trends_100$value[i], y = parallel.trends_100[i, j], 
                 xend = parallel.trends_100$value[i+1], yend = parallel.trends_100[i+1, j])
     
  }
  
  parallel.trends_100_graphic_1
}
 
parallel.trends_100_graphic_1


```

```{r Parallel Trends D300 Extension, echo=FALSE, results="asis", fig.cap="TEST", eval=FALSE}

# loading data
x <- read_csv("data/data_clean/data.turnout.csv", col_types = cols(
  .default = col_double(),
  reg = col_date(format = ""),
  s = col_character()
))

##these are the elections to look at
elections = c('vote1996','vote1998','vote2000','vote2002','vote2004')

##matrices for storing results
outmat = matrix(nrow=length(elections), ncol=4)
colnames(outmat) = c('white.treatment','white.control','black.treatment','black.control')

##use different registration cutoff here because going all the way back to 1996

# COULD JUST LOAD DATA HERE...

x <- x %>% 
  
  filter(reg < "1996-10-08") %>%
  
  filter(is.na(reg) == F)


##define a treatment and control group for each name percent
useW = x[x$whitename>=.975,]
useB = x[x$blackname>=.975,]

##set distance for parallel trends test to 200 meters, can be tested at other distances too (MAYBE THIS IS MY PROJECT)  
treatment_white = useW[useW$demo.distance <= 300,]
treatment_black = useB[useB$demo.distance <= 300,]
control_white = useW[useW$demo.distance > 300,]
control_black = useB[useB$demo.distance > 300,]     		

WtreatN = nrow(treatment_white)
BtreatN = nrow(treatment_black)
WcontN = nrow(control_white)
BcontN = nrow(control_black)
     
##test turnout across difference elections     
for(i in 1:length(elections)){
		election = elections[i]
		outmat[i,'white.treatment'] = sum(treatment_white[election],na.rm=T)/WtreatN
	  outmat[i,'black.treatment'] = sum(treatment_black[election],na.rm=T)/BtreatN
	  outmat[i,'white.control'] = sum(control_white[election],na.rm=T)/WcontN
	  outmat[i,'black.control'] = sum(control_black[election],na.rm=T)/BcontN  
}


# Data Wrangling -- Creating Tibble
	    
parallel.trends_100 = outmat

parallel.trends_100 <- as.tibble(parallel.trends_100) %>% 
  
  rename("WT" = "white.treatment","WC" = "white.control", "BT" = "black.treatment", "BC" = "black.control")  

year <- c('1996','1998','2000','2002','2004')  
  
year <- as.tibble(year) 

parallel.trends_100  <- tibble(parallel.trends_100, year)

# Building Basic Graphic 

parallel.trends_100_graphic <- parallel.trends_100  %>% 
  
  ggplot(aes(x = value)) +
  
  geom_text((aes(y = WT, label = "WT"))) +

  geom_text((aes(y = WC, label = "WC"))) +
  
  geom_text((aes(y = BT, label = "BT"))) +
   
  geom_text((aes(y = BC, label = "BC"))) +

  # from = .5, to = .9, by = .05
  
  scale_y_continuous(breaks = c(0.5, 0.55, 0.60, 0.65, 0.70, 0.75, 0.8, 0.85, 0.9), labels = c(0.5, 0.55, 0.60, 0.65, 0.70, 0.75, 0.8, 0.85, 0.9), name = "Percent Voting Turnout", limits = c(0.5,0.9)) + 
  
  scale_x_discrete(breaks = c(1996, 1998, 2000, 2002, 2004), labels = c(1996, 1998, 2000, 2002, 2004), name = "Year") +
  
  labs(title = "Parallel Trends Test", subtitle = "D = 300 meters") +
  
  # geom_vline(xintercept = "2002", color = "red") +
  
  # potentially do an annotate 
  
  theme_classic()  
  
parallel.trends_100_graphic_1 <- parallel.trends_100_graphic
bonjour <- c(1,2,3,4)
parallel.trends_100 <- as.data.frame(parallel.trends_100)

# Getting the Lines to Show Up, Shouldn't Have Been Its Own Thing BUT HERE WE ARE

for (j in bonjour) {
  for(i in bonjour){
     
     parallel.trends_100_graphic_1 <- parallel.trends_100_graphic_1 +  
     
    geom_segment(x = parallel.trends_100$value[i], y = parallel.trends_100[i, j], 
                 xend = parallel.trends_100$value[i+1], yend = parallel.trends_100[i+1, j])
     
  }
  
  parallel.trends_100_graphic_1
}
 
parallel.trends_100_graphic_1


```

# Conclusion

```{r FigureA2, eval = FALSE, fig.height=3, fig.width=1, fig.align="CENTER", fig.caption = "NO CAP", echo=FALSE}
knitr::include_graphics("data/images/fig2a.png")
```



Bibliography

